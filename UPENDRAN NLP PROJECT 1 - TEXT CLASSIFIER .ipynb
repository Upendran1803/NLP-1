{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a59629df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54841b5",
   "metadata": {},
   "source": [
    "#### Read the csv using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6c8252dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('blogtext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "78011aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  2059027   male   15            Student       Leo   12,May,2004   \n",
       "3  2059027   male   15            Student       Leo   12,May,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "\n",
       "                                                text  \n",
       "0             Info has been found (+/- 100 pages,...  \n",
       "1             These are the team members:   Drewe...  \n",
       "2             In het kader van kernfusie op aarde...  \n",
       "3                   testing!!!  testing!!!            \n",
       "4               Thanks to Yahoo!'s Toolbar I can ...  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # sample dataset lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "cfc1044a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(681284, 7)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape #shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "dd04ab7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         int64\n",
       "gender    object\n",
       "age        int64\n",
       "topic     object\n",
       "sign      object\n",
       "date      object\n",
       "text      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes # datatypes of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369cb93",
   "metadata": {},
   "source": [
    "#### Check if there is any null value, and get the total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "2dccdd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "gender    0\n",
       "age       0\n",
       "topic     0\n",
       "sign      0\n",
       "date      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() #no nnull values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "93946e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(10000) #let consider the first 3000 row data for model development "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac18c8e",
   "metadata": {},
   "source": [
    "#### Data Pre Processing\n",
    "\n",
    "- Remove unwanted characters\n",
    "- Convert text to lowercase\n",
    "- Remove unwanted spaces\n",
    "- Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "5e6cfc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "27460930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1705136</td>\n",
       "      <td>female</td>\n",
       "      <td>25</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>19,May,2004</td>\n",
       "      <td>take me home with you forever where I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1705136</td>\n",
       "      <td>female</td>\n",
       "      <td>25</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>23,June,2004</td>\n",
       "      <td>seductive secretness behind doors warn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1705136</td>\n",
       "      <td>female</td>\n",
       "      <td>25</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>21,June,2004</td>\n",
       "      <td>For being so kind to me when I need yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1705136</td>\n",
       "      <td>female</td>\n",
       "      <td>25</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>blurry outside sounds as people mingle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1705136</td>\n",
       "      <td>female</td>\n",
       "      <td>25</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>07,June,2004</td>\n",
       "      <td>my body feels broken while my mind rej...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  gender  age              topic      sign          date  \\\n",
       "0     2059027    male   15            Student       Leo   14,May,2004   \n",
       "1     2059027    male   15            Student       Leo   13,May,2004   \n",
       "2     2059027    male   15            Student       Leo   12,May,2004   \n",
       "3     2059027    male   15            Student       Leo   12,May,2004   \n",
       "4     3581210    male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "...       ...     ...  ...                ...       ...           ...   \n",
       "9995  1705136  female   25             indUnk    Pisces   19,May,2004   \n",
       "9996  1705136  female   25             indUnk    Pisces  23,June,2004   \n",
       "9997  1705136  female   25             indUnk    Pisces  21,June,2004   \n",
       "9998  1705136  female   25             indUnk    Pisces  09,June,2004   \n",
       "9999  1705136  female   25             indUnk    Pisces  07,June,2004   \n",
       "\n",
       "                                                   text  \n",
       "0                Info has been found (+/- 100 pages,...  \n",
       "1                These are the team members:   Drewe...  \n",
       "2                In het kader van kernfusie op aarde...  \n",
       "3                      testing!!!  testing!!!            \n",
       "4                  Thanks to Yahoo!'s Toolbar I can ...  \n",
       "...                                                 ...  \n",
       "9995          take me home with you forever where I ...  \n",
       "9996          seductive secretness behind doors warn...  \n",
       "9997          For being so kind to me when I need yo...  \n",
       "9998          blurry outside sounds as people mingle...  \n",
       "9999          my body feels broken while my mind rej...  \n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "0c19702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only alphabets\n",
    "import re\n",
    "df_clean['text_clean'] = df_clean.text.apply(lambda x: re.sub('[^A-Za-z]+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "bf7310bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "df_clean['text_clean'] = df_clean.text_clean.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4a15242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip unwanted spaces\n",
    "df_clean['text_clean'] = df_clean.text_clean.apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "633b069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "df_clean['text_clean'] = df_clean.text_clean.apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2ce3a6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "      <td>info found pages mb pdf files wait untill team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "      <td>team members drewes van der laag urllink mail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "      <td>testing testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "      <td>thanks yahoo toolbar capture urls popups means...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>take me home with you forever where I ...</td>\n",
       "      <td>take home forever may rest sleep arms forgotte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>seductive secretness behind doors warn...</td>\n",
       "      <td>seductive secretness behind doors warning neve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>For being so kind to me when I need yo...</td>\n",
       "      <td>kind need holding hand petting hair cry bring ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>blurry outside sounds as people mingle...</td>\n",
       "      <td>blurry outside sounds people mingle pass darkn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>my body feels broken while my mind rej...</td>\n",
       "      <td>body feels broken mind rejoices thought warmth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0                Info has been found (+/- 100 pages,...   \n",
       "1                These are the team members:   Drewe...   \n",
       "2                In het kader van kernfusie op aarde...   \n",
       "3                      testing!!!  testing!!!             \n",
       "4                  Thanks to Yahoo!'s Toolbar I can ...   \n",
       "...                                                 ...   \n",
       "9995          take me home with you forever where I ...   \n",
       "9996          seductive secretness behind doors warn...   \n",
       "9997          For being so kind to me when I need yo...   \n",
       "9998          blurry outside sounds as people mingle...   \n",
       "9999          my body feels broken while my mind rej...   \n",
       "\n",
       "                                             text_clean  \n",
       "0     info found pages mb pdf files wait untill team...  \n",
       "1     team members drewes van der laag urllink mail ...  \n",
       "2     het kader van kernfusie op aarde maak je eigen...  \n",
       "3                                       testing testing  \n",
       "4     thanks yahoo toolbar capture urls popups means...  \n",
       "...                                                 ...  \n",
       "9995  take home forever may rest sleep arms forgotte...  \n",
       "9996  seductive secretness behind doors warning neve...  \n",
       "9997  kind need holding hand petting hair cry bring ...  \n",
       "9998  blurry outside sounds people mingle pass darkn...  \n",
       "9999  body feels broken mind rejoices thought warmth...  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df_clean[['text','text_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066c7cd",
   "metadata": {},
   "source": [
    "**From the above comparsion we have clean the data in every aspects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "2cc0b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop('text',axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "1caf1d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "34fa2df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ah korean language looks difficult first figure read hanguel korea surprisingly easy learn alphabet characters seems easy vocabulary starts oh backwards us sentence structure yikes luckily many options us slow witted foreigners take language course could list urllink joongang article says lot resources urllink well guy motivation jeon ji hyun latest something actually star movies cfs hear means commercial feature positive saw latest movie sunday night hard describe name english version windstruck korean version yeochinso short ne yeojachingu rul sogayhamnida like introduce girlfriend surprisingly titles make sense like website korean english looks quite good actually urllink movie shown theatres subtitles special times info urllink list many theatres seoul click urllink urllink great reason learn korean already married went foreigners well local korean national course korean take picture put urllink movie hof bar update bud mine passed urllink link giordano ad apparently aired korea nothing xxx sensibilities sort'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text_clean[10] #clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d20126c",
   "metadata": {},
   "source": [
    "#### Merge the other columns to have dependant and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "06cfe88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = df.apply(lambda row: [row['gender'], row['topic']], axis=1) #Since we need to identify the author\n",
    "#we are considering the gender and topic columns and mergin into labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "65e34aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text_clean','labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "a2897ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info found pages mb pdf files wait untill team...</td>\n",
       "      <td>[male, Student]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>team members drewes van der laag urllink mail ...</td>\n",
       "      <td>[male, Student]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "      <td>[male, Student]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing testing</td>\n",
       "      <td>[male, Student]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thanks yahoo toolbar capture urls popups means...</td>\n",
       "      <td>[male, InvestmentBanking]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_clean  \\\n",
       "0  info found pages mb pdf files wait untill team...   \n",
       "1  team members drewes van der laag urllink mail ...   \n",
       "2  het kader van kernfusie op aarde maak je eigen...   \n",
       "3                                    testing testing   \n",
       "4  thanks yahoo toolbar capture urls popups means...   \n",
       "\n",
       "                      labels  \n",
       "0            [male, Student]  \n",
       "1            [male, Student]  \n",
       "2            [male, Student]  \n",
       "3            [male, Student]  \n",
       "4  [male, InvestmentBanking]  "
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #final dataset for model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43b618",
   "metadata": {},
   "source": [
    "## Create training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "2335a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text_clean.values, df.labels.values, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "1c064258",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boston behalf great state illinois crossroads nation land lincoln let express deep gratitude privilege addressing convention tonight particular honor let face presence stage pretty unlikely father foreign student born raised small village kenya grew herding goats went school tin roof shack father grandfather cook domestic servant grandfather larger dreams son hard work perseverance father got scholarship study magical place america stood beacon freedom opportunity many come studying father met mother born town side world kansas father worked oil rigs farms depression day pearl harbor signed duty joined patton army marched across europe back home grandmother raised baby went work bomber assembly line war studied gi bill bought house fha moved west search opportunity big dreams daughter common dream born two continents parents shared improbable love shared abiding faith possibilities nation would give african name barack blessed believing tolerant america name barrier success imagined going best schools land even though rich generous america rich achieve potential passed away yet know night look pride stand today grateful diversity heritage aware parents dreams live precious daughters stand knowing story part larger american story owe debt came country earth story even possible tonight gather affirm greatness nation height skyscrapers power military size economy pride based simple premise summed declaration made two hundred years ago hold truths self evident men created equal endowed creator certain inalienable rights among life liberty pursuit happiness true genius america faith simple dreams people insistence small miracles tuck children night know fed clothed safe harm say think write think without hearing sudden knock door idea start business without paying bribe hiring somebody son participate political process without fear retribution votes counted least time year election called reaffirm values commitments hold hard reality see measuring legacy forbearers promise future generations fellow americans democrats republicans independents say tonight work workers met galesburg illinois losing union jobs maytag plant moving mexico compete children jobs pay seven bucks hour father met losing job choking back tears wondering would pay month drugs son needs without health benefits counted young woman east st louis thousands like grades drive money go college get wrong people meet small towns big cities diners office parks expect government solve problems know work hard get ahead want go collar counties around chicago people tell want tax money wasted welfare agency pentagon go inner city neighborhood folks tell government alone teach kids learn know parents parent children achieve unless raise expectations turn television sets eradicate slander says black youth book acting white people expect government solve problems sense deep bones change priorities make sure every child america decent shot life doors opportunity remain open know better want choice election offer choice party chosen man lead us embodies best country offer man john kerry john kerry understands ideals community faith sacrifice defined life heroic service vietnam years prosecutor lieutenant governor two decades united states senate devoted country seen make tough choices easier ones available values record affirm best us john kerry believes america hard work rewarded instead offering tax breaks companies shipping jobs overseas offer companies creating jobs home john kerry believes america americans afford health coverage politicians washington john kerry believes energy independence held hostage profits oil companies sabotage foreign oil fields john kerry believes constitutional freedoms made country envy world never sacrifice basic liberties use faith wedge divide us john kerry believes dangerous world war must option never first option back met young man named shamus vfw hall east moline illinois good looking kid six two six three clear eyed easy smile told joined marines heading iraq following week listened explain enlisted absolute faith country leaders devotion duty service thought young man us might hope child asked serving shamus well serving us thought service men women sons daughters husbands wives friends neighbors returning hometowns thought families met struggling get without loved one full income whose loved ones returned limb missing nerves shattered still lacked long term health benefits reservists send young men women harm way solemn obligation fudge numbers shade truth going care families gone tend soldiers upon return never ever go war without enough troops win war secure peace earn respect world let clear real enemies world enemies must found must pursued must defeated john kerry knows lieutenant kerry hesitate risk life protect men served vietnam president kerry hesitate one moment use military might keep america safe secure john kerry believes america knows enough us prosper alongside famous individualism another ingredient american saga belief connected one people child south side chicago read matters even child senior citizen somewhere pay prescription choose medicine rent makes life poorer even grandmother arab american family rounded without benefit attorney due process threatens civil liberties fundamental belief brother keeper sisters keeper makes country work allows us pursue individual dreams yet still come together single american family e pluribus unum many one yet even speak preparing divide us spin masters negative ad peddlers embrace politics anything goes well say tonight liberal america conservative america united states america black america white america latino america asian america united states america pundits like slice dice country red states blue states red states republicans blue states democrats got news worship awesome god blue states like federal agents poking around libraries red states coach little league blue states gay friends red states patriots opposed war iraq patriots supported one people us pledging allegiance stars stripes us defending united states america end election participate politics cynicism politics hope john kerry calls us hope john edwards calls us hope talking blind optimism almost willful ignorance thinks unemployment go away talk health care crisis solve ignore talking something substantial hope slaves sitting around fire singing freedom songs hope immigrants setting distant shores hope young naval lieutenant bravely patrolling mekong delta hope millworker son dares defy odds hope skinny kid funny name believes america place audacity hope end god greatest gift us bedrock nation belief things seen belief better days ahead believe give middle class relief provide working families road opportunity believe provide jobs jobless homes homeless reclaim young people cities across america violence despair believe stand crossroads history make right choices meet challenges face us america tonight feel energy urgency passion hopefulness must doubt across country florida oregon washington maine people rise november john kerry sworn president john edwards sworn vice president country reclaim promise long political darkness brighter day come thank god bless'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dacf709",
   "metadata": {},
   "source": [
    "## Vectorize the data\n",
    "\n",
    "### Create Bag of Words\n",
    "- Use CountVectorizer\n",
    "- Transform the traing and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "2032b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "9883de08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aa amazing', 'aa anger', 'aa compared', 'aa keeps',\n",
       "       'aa nice', 'aa sd', 'aaa', 'aaa come', 'aaa discount'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "589e9d49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train_bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81903da8",
   "metadata": {},
   "source": [
    "### Create a dictionary to get label counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "a7d0f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = dict()\n",
    "\n",
    "for labels in df.labels.values:\n",
    "    for label in labels:\n",
    "        if label in label_counts:\n",
    "            label_counts[label] += 1\n",
    "        else:\n",
    "            label_counts[label] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "82b60a18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'male': 5916,\n",
       " 'Student': 1137,\n",
       " 'InvestmentBanking': 70,\n",
       " 'female': 4084,\n",
       " 'indUnk': 3287,\n",
       " 'Non-Profit': 71,\n",
       " 'Banking': 16,\n",
       " 'Education': 270,\n",
       " 'Engineering': 127,\n",
       " 'Science': 63,\n",
       " 'Communications-Media': 99,\n",
       " 'BusinessServices': 91,\n",
       " 'Sports-Recreation': 80,\n",
       " 'Arts': 45,\n",
       " 'Internet': 118,\n",
       " 'Museums-Libraries': 17,\n",
       " 'Accounting': 4,\n",
       " 'Technology': 2654,\n",
       " 'Law': 11,\n",
       " 'Consulting': 21,\n",
       " 'Automotive': 14,\n",
       " 'Religion': 9,\n",
       " 'Fashion': 1622,\n",
       " 'Publishing': 4,\n",
       " 'Marketing': 156,\n",
       " 'LawEnforcement-Security': 10,\n",
       " 'HumanResources': 2,\n",
       " 'Telecommunications': 2}"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd5b5c",
   "metadata": {},
   "source": [
    "## Multi label binarizer\n",
    "\n",
    "Load a multilabel binarizer and fit it on the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "f5467dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=sorted(label_counts.keys()))\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_test = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ec1fe8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf = OneVsRestClassifier(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bdce2",
   "metadata": {},
   "source": [
    "### Fit the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "34746100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression())"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90093fd4",
   "metadata": {},
   "source": [
    "### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "18ce5142",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = clf.predict(X_test_bow)\n",
    "predicted_scores = clf.decision_function(X_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c709f3df",
   "metadata": {},
   "source": [
    "### Get inverse transform for predicted labels and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "8fe27078",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_inversed = mlb.inverse_transform(predicted_labels)\n",
    "y_test_inversed = mlb.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb669f",
   "metadata": {},
   "source": [
    "### Print some samples prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "55170c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\tthanks stopping blog goal site place mostly vent corp bullshit tell stupid drunken stories host various ramblings peace eric\n",
      "True labels:\tConsulting,male\n",
      "Predicted labels:\tmale\n",
      "\n",
      "\n",
      "Title:\tmorning kinda forgot football today surpising since like every day except sunday next months something forgot football also forgot time football unbearable state affairs assure attempted mad dash stairs brought startlingly short fact legs want listen decided listen someone else someone completely unrelated also quite invisible told time would much better spent spasming legs want sorta quiver floor instead actually sprinting bodily mutany forced rely upon trusty much loyal arms transportation basement lair went well came door ever tried opening door top stairs without able use legs quite tricky end propped leaned hard managed stroke handle enough somehow pop open victory last one small problem football schedule fridge high fridge normally eye level towering frame eyes reduced midgethood ground small even one little people makes rather difficult read things fridge feet head well reading things feet head fridge made point font always hard people foot fridges football schedules run problem every day brilliant mind came rescue stools kind leave toilette turncoat mafia member smell slimy skin high value street wooden kind normally would perch upon dine luxerious countertop kitchen two placed appropriate distance apart easily able lift onto sitting position upon third kinda like really monsterous dip dogs staring shock legs still little dance high mighty vantage point shocked assertain practice started good hour ago wait mind screamed today could rd simply watched casablanca yet absolutely knew due rd faced cold unforgiving logic brain forced admit today probably rd furthermore phoney thing clearly said th either could football practice th practice reason legs badly misbehaving crawl upstairs check practice started point legs decided invisible stranger led astray true path body part real hope enter divine heaven sitting lazy boy giant glass iced lemonade watching bond movie eating ribs course legs never say never eaten rib drunk lemonade even seen james bond however understand blissful state certainly enjoy sitting lazy boy running leads beleive divine heaven must nice place hey everyone else seems excited allowed return arms normal role hanging sides occasionally turning doornobs reaching remotes lightswitches decided ordeal meant entire body deserved dip semi divine heaven warm bath lots bubbles soothing music smelling salts though pretty sure smelling salts figured could fake nose fragrant bubble stuff maybe nice candle face flight stairs obtain goal one could asend humans humans paralyzing back injuries elevators kind people carry however walked past front door looked window window could see inside car window letting see inside car could see throught window going car throught window going car could see neighbor window across street window across street beheld strange sight indeed year old blonde streetmate dancing sort weird macarana like dance thought already left college least moved year old hippie wow people really shut blinds one talk though blinds glass door basement love dancing around skivies one dumb enough risk peering windows\n",
      "True labels:\tindUnk,male\n",
      "Predicted labels:\tmale\n",
      "\n",
      "\n",
      "Title:\tgot another promotion work sorta going working front desk distribution deliveries billing inside business business publication style selling subscriptions get dollar one sell prospects working hours week next weeks whew love job boss walked told pleasure great job wow\n",
      "True labels:\tTechnology,male\n",
      "Predicted labels:\tmale\n",
      "\n",
      "\n",
      "Title:\tmet brian january started dating february nbsp longest boyfriend distance time started living together october nbsp love much friend lover future husband hopefully nbsp want children pressure nbsp disagreements something always seemed work thing hinders relationship fact parents hate get go hate everyone brian really hard approval never get anything fight parents anything still told live together spoken almost months nbsp really long time speak family nbsp talk family week nbsp think fact wrote letter telling parents feel really made brian mad brian cannot stand nbsp anyone stood think anything wrong except putting things writing instead calling spoken words fade away nbsp written words people dwell misery spoke truth viewpoint nbsp tried straightening thought made cry pushed away miserable people ever met entire life nbsp brian tends get sucked misery believes sometimes fault brian hard time people nbsp always wants people approval everything nbsp struggles daily basis suffocates really thinks people like nbsp line draw comes people approval nbsp think brian willing cross line sometimes nbsp becomes concerned people think irritating care much people think good bad days someone catches bad day like hide feelings nbsp brian hard time would rather suck squash nbsp think brian lot growing lot hardening well brian never job life depended money always something prevent situation understand tired sleep however much needs cannot understand wanting sit around never home always home cannot understand around people time need space think change finally completely working full time pay debts school get wrong brian wonderful person every wonderful person flaws\n",
      "True labels:\tAutomotive,female\n",
      "Predicted labels:\tfemale,indUnk\n",
      "\n",
      "\n",
      "Title:\tbirthday pampering anything better decided another thing pay pedicures went saturday birthday pedi realized like pro really nothing like made birthday resolution pedicure month sunday saw la traviatta florida grand opera another birthday pampering absolutely gorgeous costumes set music voices combined make day special love opera come lady always die end struggle suffer try live proper life make sacrifices others die horrible disease even worse hand makes girl want go bad baby brother town week holiday baby brother like football player big still baby brother nice someone around play girls instead change dying tell else weekend propiety demands keep clean suffice say interesting new friend sort ensures never die death opera heroine wink wink nuff said diva\n",
      "True labels:\tfemale,indUnk\n",
      "Predicted labels:\tfemale,indUnk\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n",
    "        X_test[i],\n",
    "        ','.join(y_test_inversed[i]),\n",
    "        ','.join(pred_inversed[i])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de71b6",
   "metadata": {},
   "source": [
    "**Model got predicited pretty good**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41897025",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "b9a6184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score,log_loss\n",
    "\n",
    "def print_evaluation_scores(y_val, predicted):\n",
    "    print('Accuracy score: ', accuracy_score(y_val, predicted))\n",
    "    print('F1 score: ', f1_score(y_val, predicted, average='micro'))\n",
    "    print('Average precision score: ', average_precision_score(y_val, predicted, average='micro'))\n",
    "    print('Average recall score: ', recall_score(y_val, predicted, average='micro'))\n",
    "#     print('Log-loss score: ', log_loss(y_val, predicted))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c132ccd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4mBag-of-words\u001b[4m\u001b[0m\n",
      "Accuracy score:  0.4405\n",
      "F1 score:  0.7114337568058076\n",
      "Average precision score:  0.5390730545142496\n",
      "Average recall score:  0.637\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m\\033[4mBag-of-words\\033[4m\\033[0m')\n",
    "print_evaluation_scores(y_test, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff77ef1",
   "metadata": {},
   "source": [
    "## Experimenting with other vectorizer \n",
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "23121a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF\n",
    "vectorizer_tf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tf = vectorizer_tf.fit_transform(X_train)\n",
    "X_test_tf = vectorizer_tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "29c6d20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aaron', 'ability', 'able', 'absence', 'absolute', 'absolutely',\n",
       "       'abt', 'abuse', 'accent', 'accept'], dtype=object)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tf.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "86ae6b37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a2c28",
   "metadata": {},
   "source": [
    "## We can build Classifier using other algorithms e.g SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "df17c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "728ad883",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc_tf_1 = OneVsRestClassifier(SVC()).fit(X_train_tf, y_train)\n",
    "# clf_svc_tf = SVC()\n",
    "# clf_svc_tf = OneVsRestClassifier(clf_svc_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "157e4fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_svc_tf.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "170ded70",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_ttf = clf_svc_tf_1.predict(X_test_tf)\n",
    "predicted_scores_tf = clf_svc_tf_1.decision_function(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "928254f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_inversed_tf = mlb.inverse_transform(predicted_labels_ttf)\n",
    "y_test_inversed_tf = mlb.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "0c02a035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\tthanks stopping blog goal site place mostly vent corp bullshit tell stupid drunken stories host various ramblings peace eric\n",
      "True labels:\tConsulting,male\n",
      "Predicted labels:\tmale\n",
      "\n",
      "\n",
      "Title:\tmorning kinda forgot football today surpising since like every day except sunday next months something forgot football also forgot time football unbearable state affairs assure attempted mad dash stairs brought startlingly short fact legs want listen decided listen someone else someone completely unrelated also quite invisible told time would much better spent spasming legs want sorta quiver floor instead actually sprinting bodily mutany forced rely upon trusty much loyal arms transportation basement lair went well came door ever tried opening door top stairs without able use legs quite tricky end propped leaned hard managed stroke handle enough somehow pop open victory last one small problem football schedule fridge high fridge normally eye level towering frame eyes reduced midgethood ground small even one little people makes rather difficult read things fridge feet head well reading things feet head fridge made point font always hard people foot fridges football schedules run problem every day brilliant mind came rescue stools kind leave toilette turncoat mafia member smell slimy skin high value street wooden kind normally would perch upon dine luxerious countertop kitchen two placed appropriate distance apart easily able lift onto sitting position upon third kinda like really monsterous dip dogs staring shock legs still little dance high mighty vantage point shocked assertain practice started good hour ago wait mind screamed today could rd simply watched casablanca yet absolutely knew due rd faced cold unforgiving logic brain forced admit today probably rd furthermore phoney thing clearly said th either could football practice th practice reason legs badly misbehaving crawl upstairs check practice started point legs decided invisible stranger led astray true path body part real hope enter divine heaven sitting lazy boy giant glass iced lemonade watching bond movie eating ribs course legs never say never eaten rib drunk lemonade even seen james bond however understand blissful state certainly enjoy sitting lazy boy running leads beleive divine heaven must nice place hey everyone else seems excited allowed return arms normal role hanging sides occasionally turning doornobs reaching remotes lightswitches decided ordeal meant entire body deserved dip semi divine heaven warm bath lots bubbles soothing music smelling salts though pretty sure smelling salts figured could fake nose fragrant bubble stuff maybe nice candle face flight stairs obtain goal one could asend humans humans paralyzing back injuries elevators kind people carry however walked past front door looked window window could see inside car window letting see inside car could see throught window going car throught window going car could see neighbor window across street window across street beheld strange sight indeed year old blonde streetmate dancing sort weird macarana like dance thought already left college least moved year old hippie wow people really shut blinds one talk though blinds glass door basement love dancing around skivies one dumb enough risk peering windows\n",
      "True labels:\tindUnk,male\n",
      "Predicted labels:\tfemale\n",
      "\n",
      "\n",
      "Title:\tgot another promotion work sorta going working front desk distribution deliveries billing inside business business publication style selling subscriptions get dollar one sell prospects working hours week next weeks whew love job boss walked told pleasure great job wow\n",
      "True labels:\tTechnology,male\n",
      "Predicted labels:\tmale\n",
      "\n",
      "\n",
      "Title:\tmet brian january started dating february nbsp longest boyfriend distance time started living together october nbsp love much friend lover future husband hopefully nbsp want children pressure nbsp disagreements something always seemed work thing hinders relationship fact parents hate get go hate everyone brian really hard approval never get anything fight parents anything still told live together spoken almost months nbsp really long time speak family nbsp talk family week nbsp think fact wrote letter telling parents feel really made brian mad brian cannot stand nbsp anyone stood think anything wrong except putting things writing instead calling spoken words fade away nbsp written words people dwell misery spoke truth viewpoint nbsp tried straightening thought made cry pushed away miserable people ever met entire life nbsp brian tends get sucked misery believes sometimes fault brian hard time people nbsp always wants people approval everything nbsp struggles daily basis suffocates really thinks people like nbsp line draw comes people approval nbsp think brian willing cross line sometimes nbsp becomes concerned people think irritating care much people think good bad days someone catches bad day like hide feelings nbsp brian hard time would rather suck squash nbsp think brian lot growing lot hardening well brian never job life depended money always something prevent situation understand tired sleep however much needs cannot understand wanting sit around never home always home cannot understand around people time need space think change finally completely working full time pay debts school get wrong brian wonderful person every wonderful person flaws\n",
      "True labels:\tAutomotive,female\n",
      "Predicted labels:\tfemale\n",
      "\n",
      "\n",
      "Title:\tbirthday pampering anything better decided another thing pay pedicures went saturday birthday pedi realized like pro really nothing like made birthday resolution pedicure month sunday saw la traviatta florida grand opera another birthday pampering absolutely gorgeous costumes set music voices combined make day special love opera come lady always die end struggle suffer try live proper life make sacrifices others die horrible disease even worse hand makes girl want go bad baby brother town week holiday baby brother like football player big still baby brother nice someone around play girls instead change dying tell else weekend propiety demands keep clean suffice say interesting new friend sort ensures never die death opera heroine wink wink nuff said diva\n",
      "True labels:\tfemale,indUnk\n",
      "Predicted labels:\tfemale,indUnk\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n",
    "        X_test[i],\n",
    "        ','.join(y_test_inversed_tf[i]),\n",
    "        ','.join(pred_inversed_tf[i])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "cea2685d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4mTF-IDF\u001b[4m\u001b[0m\n",
      "Accuracy score:  0.3605\n",
      "F1 score:  0.688231850117096\n",
      "Average precision score:  0.517370245661824\n",
      "Average recall score:  0.58775\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m\\033[4mTF-IDF\\033[4m\\033[0m')\n",
    "print_evaluation_scores(y_test, predicted_labels_ttf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35ecbc",
   "metadata": {},
   "source": [
    "   class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92c694",
   "metadata": {},
   "source": [
    "### Insights on Comparing two vectorizer \n",
    "\n",
    "- On comparing both count vectorizer gave a good F1 score 71% againt Tf-idf which is 69% (for dataset of 10000 rows)\n",
    "\n",
    "- Count Vectorizer perform well because of n-gram fitted into it, so model could learn more vocab.\n",
    "\n",
    "- Logistics Regression outperformered well compared to SVM. Since SVM doesn’t support multiclass classification natively. we convert it one-to-rest, them=n we used svm on dataset.\n",
    "\n",
    "- F1 score should be consider the most important matrix in multiclass label. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29287390",
   "metadata": {},
   "source": [
    "# Part 2 - Chat bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11dbdbb",
   "metadata": {},
   "source": [
    "**Load our GL json file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "6ca4b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "\n",
    "import json\n",
    "with open('GL bot.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "#Now its time to take out the data we want from our JSON file. We need all of the patterns and\n",
    "#which class/tag they belong to. We also want a list of all of the unique words in our patterns\n",
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "#Now its time to loop through our JSON data and extract the data we want. \n",
    "#For each pattern we will turn it into a list of words using nltk.word_tokenizer, rather than having them as strings. \n",
    "#We will then add each pattern into our docs_x list and its associated tag into the docs_y list\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "        \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b9cc7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "labels = sorted(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd27cf",
   "metadata": {},
   "source": [
    "**Creat Bag of words and making train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ca21be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "\n",
    "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "    \n",
    "training = numpy.array(training)\n",
    "output = numpy.array(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff502f2",
   "metadata": {},
   "source": [
    "## Develop a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9c3d1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.compat.v1.reset_default_graph()\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 32)\n",
    "net = tflearn.fully_connected(net, 16)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "5e4c3aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 22999  | total loss: \u001b[1m\u001b[32m0.44479\u001b[0m\u001b[0m | time: 0.056s\n",
      "| Adam | epoch: 1000 | loss: 0.44479 - acc: 0.9704 -- iter: 176/184\n",
      "Training Step: 23000  | total loss: \u001b[1m\u001b[32m0.40032\u001b[0m\u001b[0m | time: 0.058s\n",
      "| Adam | epoch: 1000 | loss: 0.40032 - acc: 0.9733 -- iter: 184/184\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\AI_SG\\Documents\\Upendran\\Course\\python\\Projects\\NLP\\W3 NLP Projects\\DATA\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save(\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ab6b91e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\AI_SG\\Documents\\Upendran\\Course\\python\\Projects\\NLP\\W3 NLP Projects\\DATA\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load(\"model.tflearn\")\n",
    "except:\n",
    "    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "    model.save(\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c17a7",
   "metadata": {},
   "source": [
    "**Making Predictions:**\n",
    "\n",
    "*Now its time to actually use the model! Ideally we want to generate a response to any sentence the user types in. To do this we need to remember that our model does not take string input, it takes a bag of words. We also need to realize that our model does not spit out sentences, it generates a list of probabilities for all of our classes. This makes the process to generate a response look like the following:*\n",
    "\n",
    "    – Get some input from the user\n",
    "    – Convert it to a bag of words\n",
    "    – Get a prediction from the model\n",
    "    – Find the most probable class\n",
    "    – Pick a response from that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "86619f95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I'm Bot. Hope your day is good.May I know what you are looking for ?\n",
      "Note: type end to stop!\n",
      "You: hai\n",
      "Hi Welcome you to Great Learning Virtual Assistant! how can i help you ?\n",
      "You: what is olympus\n",
      "Usr this link to know more about olympus: https://olympus.mygreatlearning.com/courses/52238/pages/olympus-2-dot-0?module_item_id=1374482 \n",
      "You: i need to learn about Machine learning\n",
      "https://www.simplilearn.com/10-algorithms-machine-learning-engineers-need-to-know-article\n",
      "You: what is KNN?\n",
      "Kindly visit the link to know more:: https://en.wikipedia.org/wiki/Machine_learning \n",
      "You: What are my career opportunities in this AIML fields?\n",
      "Transferring the request to your PM, Please hold on\n",
      "You: my career path\n",
      "Follow the link to know more: https://www.mygreatlearning.com/blog/\n",
      "You: top free course in GL\n",
      "Transferring the request to your PM, Please hold on\n",
      "You: free course\n",
      "You can able to access our free course here : https://www.mygreatlearning.com/academy/learn-for-free/courses\n",
      "You: trending news in AIML\n",
      "Follow the link to know more: https://www.mygreatlearning.com/blog/\n",
      "You: can you help me to solve the problem?\n",
      "Transferring the request to your PM, Please hold on\n",
      "You: can you help me\n",
      "You can able to access our free course here : https://www.mygreatlearning.com/academy/learn-for-free/courses\n",
      "You: i need help\n",
      "Tell me your problem to assist you\n",
      "You: end\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)\n",
    "\n",
    "\n",
    "def chat():\n",
    "    print(\"Hi! I'm Bot. Hope your day is good.May I know what you are looking for ?\\n\" \"Note: type end to stop!\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"end\":\n",
    "            break\n",
    "\n",
    "        results = model.predict([bag_of_words(inp, words)])\n",
    "        results_index = numpy.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "\n",
    "        print(random.choice(responses))\n",
    "\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb4626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
